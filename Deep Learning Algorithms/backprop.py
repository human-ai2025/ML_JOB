"""
NOTES
1. FOR EVERY OPERATIONS WE PERFORM ON TENSORS, PYTORCH WILL CREATE
A COMPUTATIONAL GRAPH FOR US
2. AT THESE NODES WE CAN CALCULATE THE LOCAL GRADIENTS AND AT FINAL WE
CAN CREATE THE FINAL GRADIENT
3. AT THE VERY END WE CALCULATE A LOSS FUNCTION THAT WE NEED TO MINIMIZE
4. THEN WE CALCULATE THE GRADIENTS OF THE LOSS WITH RESPECT TO THE PARAMETERS

THERE ARE BASICALLY 3 STEPS
1. FORWARD PASS: COMPUTE THE LOSS
2. COMPUTE THE LOCAL GRADIENT
3. BACKWARD PASS: COMPUTE THE GRADIENT OF LOSS WITH RESPECT TO THE WEIGHTS


LETS TAKE AN EXAMPLE OF LINEAR REGRESSION

Y_PREDICTED = WEIGHT.INPUT_X
LOSS = (Y_PREDICTED - Y_ACTUAL)^2
MINIMIZE THE LOSS

"""

import torch

x = torch.tensor(1.0)
y = torch.tensor(2.0)

w = torch.tensor(1.0, requires_grad=True)

# forward  pass
y_hat = w * x
loss = (y_hat - y)**2

print(loss)

# backward pass
loss.backward()
print(w.grad)


# update the weights
# next forward and backward pass and perfrm for a next few iteration